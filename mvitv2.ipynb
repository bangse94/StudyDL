{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Project input to patch\n",
    "- 입력 이미지를 패치로 나눠준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  torch.Size([8, 3, 224, 224])\n",
      "patches:  torch.Size([8, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "x = torch.randn(8, 3, 224, 224)\n",
    "print('x: ', x.shape)\n",
    "\n",
    "patch_size = 16 # 16x16 patches\n",
    "patches = rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size)\n",
    "print('patches: ', patches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 reaarange를 통해 쉽게 reshape 할 수 있지만, 아래와 같이 Conv를 이용해 패치를 구성하면 성능상 이점이 있다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 2.25\n",
      "Estimated Total Size (MB): 5.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "patch_size = 16\n",
    "in_channels = 3\n",
    "embed_dim = 768\n",
    "\n",
    "projection = nn.Sequential(\n",
    "    nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size),\n",
    "    Rearrange('b e (h) (w) -> b (h w) e')\n",
    ")\n",
    "\n",
    "summary(projection, x.shape[1:], device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Patch embedding\n",
    "- patches에 class token과 positional embedding을 추가한다.\n",
    "- class toke은 어떤 클래스인지 학습하기 위한 파라미터이다.\n",
    "- Positional은 패치가 어떤 위치에 있는지 알기 위한 학습 가능한 파라미터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected X shape : torch.Size([8, 196, 768])\n",
      "Cls Shape : torch.Size([1, 1, 768]), Position Shape : torch.Size([197, 768])\n",
      "Repeated Cls shape : torch.Size([8, 1, 768])\n",
      "Cat X shape : torch.Size([8, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 768\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "\n",
    "projected_x = projection(x)\n",
    "print(f'Projected X shape : {projected_x.shape}')\n",
    "\n",
    "cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "positions = nn.Parameter(torch.randn((img_size // patch_size) ** 2 + 1, embed_dim))\n",
    "print(f\"Cls Shape : {cls_token.shape}, Position Shape : {positions.shape}\")\n",
    "\n",
    "batch_size = 8\n",
    "cls_tokens = repeat(cls_token, '() n e -> b n e', b=batch_size)\n",
    "print(f\"Repeated Cls shape : {cls_tokens.shape}\")\n",
    "\n",
    "cat_x = torch.cat([cls_tokens, projected_x], dim= 1)\n",
    "\n",
    "cat_x += positions\n",
    "print(f\"Cat X shape : {cat_x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "================================================================\n",
      "Total params: 590,592\n",
      "Trainable params: 590,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 2.25\n",
      "Estimated Total Size (MB): 5.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 16, embed_dim: int = 768, img_size: int = 224):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e')\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) ** 2 + 1, embed_dim))\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        \n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        \n",
    "        x += self.positions\n",
    "        \n",
    "        return x\n",
    "    \n",
    "PE = PatchEmbedding()\n",
    "summary(PE, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-head Self-Attention(MHA)\n",
    "- 패치들에 대해 셀프 어텐션 메커니즘을 적용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=768, bias=True), Linear(in_features=768, out_features=768, bias=True), Linear(in_features=768, out_features=768, bias=True)\n",
      "torch.Size([8, 197, 768])\n",
      "shape : torch.Size([8, 8, 197, 96]), torch.Size([8, 8, 197, 96]), torch.Size([8, 8, 197, 96])\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 768\n",
    "num_heads = 8\n",
    "\n",
    "keys    = nn.Linear(embed_dim, embed_dim)\n",
    "queries = nn.Linear(embed_dim, embed_dim)\n",
    "values  = nn.Linear(embed_dim, embed_dim)\n",
    "print(f\"{keys}, {queries}, {values}\")\n",
    "\n",
    "x = PE(x)\n",
    "print(f\"{queries(x).shape}\")\n",
    "\n",
    "queries = rearrange(queries(x), \"b n (h d) -> b h n d\", h=num_heads) # -> batch, heads, n, embed_dim\n",
    "keys    = rearrange(keys(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "values  = rearrange(values(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "\n",
    "print(f\"shape : {queries.shape}, {keys.shape}, {values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy : torch.Size([8, 8, 197, 197])\n",
      "att : torch.Size([8, 8, 197, 197])\n",
      "out : torch.Size([8, 8, 197, 96])\n",
      "out2 : torch.Size([8, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# Queries * Keys\n",
    "energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "print(f\"energy : {energy.shape}\")\n",
    "\n",
    "# Get Attention Score\n",
    "scaling = embed_dim ** (1/2)\n",
    "att = F.softmax(energy / scaling, dim=-1)\n",
    "print(f\"att : {att.shape}\")\n",
    "\n",
    "# Attention Score * values\n",
    "out = torch.einsum('bhal, bhlv -> bhav', att, values)\n",
    "print(f\"out : {out.shape}\")\n",
    "\n",
    "# Rearrange to embed_dim\n",
    "out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "print(f\"out2 : {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape : torch.Size([8, 197, 768])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-2          [-1, 8, 197, 197]               0\n",
      "            Linear-3             [-1, 197, 768]         590,592\n",
      "================================================================\n",
      "Total params: 2,362,368\n",
      "Trainable params: 2,362,368\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.58\n",
      "Forward/backward pass size (MB): 6.99\n",
      "Params size (MB): 9.01\n",
      "Estimated Total Size (MB): 16.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "        \n",
    "        scaling = self.embed_dim ** (1/2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        \n",
    "        out = torch.einsum('bhal, bhlv -> bhav', att, values)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.projection(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "x = torch.randn(8, 3, 224, 224)\n",
    "PE = PatchEmbedding()\n",
    "x  = PE(x)\n",
    "print(f\"x shape : {x.shape}\")\n",
    "MHA = MultiHeadAttention()\n",
    "summary(MHA, x.shape[1:], device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transformer Encoder Block\n",
    "- MLP(feed forward) 블록을 만들어주고 MHA와 하나로 묶어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1             [-1, 197, 768]           1,536\n",
      "            Linear-2            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-3          [-1, 8, 197, 197]               0\n",
      "            Linear-4             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-5             [-1, 197, 768]               0\n",
      "           Dropout-6             [-1, 197, 768]               0\n",
      "       ResidualAdd-7             [-1, 197, 768]               0\n",
      "         LayerNorm-8             [-1, 197, 768]           1,536\n",
      "            Linear-9            [-1, 197, 3072]       2,362,368\n",
      "             GELU-10            [-1, 197, 3072]               0\n",
      "          Dropout-11            [-1, 197, 3072]               0\n",
      "           Linear-12             [-1, 197, 768]       2,360,064\n",
      "          Dropout-13             [-1, 197, 768]               0\n",
      "      ResidualAdd-14             [-1, 197, 768]               0\n",
      "================================================================\n",
      "Total params: 7,087,872\n",
      "Trainable params: 7,087,872\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.58\n",
      "Forward/backward pass size (MB): 30.07\n",
      "Params size (MB): 27.04\n",
      "Estimated Total Size (MB): 57.69\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "    \n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, embed_dim: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(embed_dim, expansion * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * embed_dim, embed_dim),\n",
    "        )\n",
    "        \n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int = 768,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 **  kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(embed_dim),\n",
    "                MultiHeadAttention(embed_dim, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(embed_dim),\n",
    "                FeedForwardBlock(embed_dim, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            ))\n",
    "        )\n",
    "        \n",
    "x = torch.randn(8, 3, 224, 224)\n",
    "x = PE(x)\n",
    "x = MHA(x)\n",
    "TE = TransformerEncoderBlock()\n",
    "summary(TE, x.shape[1:], device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 모두 묶어서 ViT 빌드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "    PatchEmbedding-3             [-1, 197, 768]               0\n",
      "         LayerNorm-4             [-1, 197, 768]           1,536\n",
      "            Linear-5            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-6          [-1, 8, 197, 197]               0\n",
      "            Linear-7             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-8             [-1, 197, 768]               0\n",
      "           Dropout-9             [-1, 197, 768]               0\n",
      "      ResidualAdd-10             [-1, 197, 768]               0\n",
      "        LayerNorm-11             [-1, 197, 768]           1,536\n",
      "           Linear-12            [-1, 197, 3072]       2,362,368\n",
      "             GELU-13            [-1, 197, 3072]               0\n",
      "          Dropout-14            [-1, 197, 3072]               0\n",
      "           Linear-15             [-1, 197, 768]       2,360,064\n",
      "          Dropout-16             [-1, 197, 768]               0\n",
      "      ResidualAdd-17             [-1, 197, 768]               0\n",
      "        LayerNorm-18             [-1, 197, 768]           1,536\n",
      "           Linear-19            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-20          [-1, 8, 197, 197]               0\n",
      "           Linear-21             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-22             [-1, 197, 768]               0\n",
      "          Dropout-23             [-1, 197, 768]               0\n",
      "      ResidualAdd-24             [-1, 197, 768]               0\n",
      "        LayerNorm-25             [-1, 197, 768]           1,536\n",
      "           Linear-26            [-1, 197, 3072]       2,362,368\n",
      "             GELU-27            [-1, 197, 3072]               0\n",
      "          Dropout-28            [-1, 197, 3072]               0\n",
      "           Linear-29             [-1, 197, 768]       2,360,064\n",
      "          Dropout-30             [-1, 197, 768]               0\n",
      "      ResidualAdd-31             [-1, 197, 768]               0\n",
      "        LayerNorm-32             [-1, 197, 768]           1,536\n",
      "           Linear-33            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-34          [-1, 8, 197, 197]               0\n",
      "           Linear-35             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-36             [-1, 197, 768]               0\n",
      "          Dropout-37             [-1, 197, 768]               0\n",
      "      ResidualAdd-38             [-1, 197, 768]               0\n",
      "        LayerNorm-39             [-1, 197, 768]           1,536\n",
      "           Linear-40            [-1, 197, 3072]       2,362,368\n",
      "             GELU-41            [-1, 197, 3072]               0\n",
      "          Dropout-42            [-1, 197, 3072]               0\n",
      "           Linear-43             [-1, 197, 768]       2,360,064\n",
      "          Dropout-44             [-1, 197, 768]               0\n",
      "      ResidualAdd-45             [-1, 197, 768]               0\n",
      "        LayerNorm-46             [-1, 197, 768]           1,536\n",
      "           Linear-47            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-48          [-1, 8, 197, 197]               0\n",
      "           Linear-49             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-50             [-1, 197, 768]               0\n",
      "          Dropout-51             [-1, 197, 768]               0\n",
      "      ResidualAdd-52             [-1, 197, 768]               0\n",
      "        LayerNorm-53             [-1, 197, 768]           1,536\n",
      "           Linear-54            [-1, 197, 3072]       2,362,368\n",
      "             GELU-55            [-1, 197, 3072]               0\n",
      "          Dropout-56            [-1, 197, 3072]               0\n",
      "           Linear-57             [-1, 197, 768]       2,360,064\n",
      "          Dropout-58             [-1, 197, 768]               0\n",
      "      ResidualAdd-59             [-1, 197, 768]               0\n",
      "        LayerNorm-60             [-1, 197, 768]           1,536\n",
      "           Linear-61            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-62          [-1, 8, 197, 197]               0\n",
      "           Linear-63             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-64             [-1, 197, 768]               0\n",
      "          Dropout-65             [-1, 197, 768]               0\n",
      "      ResidualAdd-66             [-1, 197, 768]               0\n",
      "        LayerNorm-67             [-1, 197, 768]           1,536\n",
      "           Linear-68            [-1, 197, 3072]       2,362,368\n",
      "             GELU-69            [-1, 197, 3072]               0\n",
      "          Dropout-70            [-1, 197, 3072]               0\n",
      "           Linear-71             [-1, 197, 768]       2,360,064\n",
      "          Dropout-72             [-1, 197, 768]               0\n",
      "      ResidualAdd-73             [-1, 197, 768]               0\n",
      "        LayerNorm-74             [-1, 197, 768]           1,536\n",
      "           Linear-75            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-76          [-1, 8, 197, 197]               0\n",
      "           Linear-77             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-78             [-1, 197, 768]               0\n",
      "          Dropout-79             [-1, 197, 768]               0\n",
      "      ResidualAdd-80             [-1, 197, 768]               0\n",
      "        LayerNorm-81             [-1, 197, 768]           1,536\n",
      "           Linear-82            [-1, 197, 3072]       2,362,368\n",
      "             GELU-83            [-1, 197, 3072]               0\n",
      "          Dropout-84            [-1, 197, 3072]               0\n",
      "           Linear-85             [-1, 197, 768]       2,360,064\n",
      "          Dropout-86             [-1, 197, 768]               0\n",
      "      ResidualAdd-87             [-1, 197, 768]               0\n",
      "        LayerNorm-88             [-1, 197, 768]           1,536\n",
      "           Linear-89            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-90          [-1, 8, 197, 197]               0\n",
      "           Linear-91             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-92             [-1, 197, 768]               0\n",
      "          Dropout-93             [-1, 197, 768]               0\n",
      "      ResidualAdd-94             [-1, 197, 768]               0\n",
      "        LayerNorm-95             [-1, 197, 768]           1,536\n",
      "           Linear-96            [-1, 197, 3072]       2,362,368\n",
      "             GELU-97            [-1, 197, 3072]               0\n",
      "          Dropout-98            [-1, 197, 3072]               0\n",
      "           Linear-99             [-1, 197, 768]       2,360,064\n",
      "         Dropout-100             [-1, 197, 768]               0\n",
      "     ResidualAdd-101             [-1, 197, 768]               0\n",
      "       LayerNorm-102             [-1, 197, 768]           1,536\n",
      "          Linear-103            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-104          [-1, 8, 197, 197]               0\n",
      "          Linear-105             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-106             [-1, 197, 768]               0\n",
      "         Dropout-107             [-1, 197, 768]               0\n",
      "     ResidualAdd-108             [-1, 197, 768]               0\n",
      "       LayerNorm-109             [-1, 197, 768]           1,536\n",
      "          Linear-110            [-1, 197, 3072]       2,362,368\n",
      "            GELU-111            [-1, 197, 3072]               0\n",
      "         Dropout-112            [-1, 197, 3072]               0\n",
      "          Linear-113             [-1, 197, 768]       2,360,064\n",
      "         Dropout-114             [-1, 197, 768]               0\n",
      "     ResidualAdd-115             [-1, 197, 768]               0\n",
      "       LayerNorm-116             [-1, 197, 768]           1,536\n",
      "          Linear-117            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-118          [-1, 8, 197, 197]               0\n",
      "          Linear-119             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-120             [-1, 197, 768]               0\n",
      "         Dropout-121             [-1, 197, 768]               0\n",
      "     ResidualAdd-122             [-1, 197, 768]               0\n",
      "       LayerNorm-123             [-1, 197, 768]           1,536\n",
      "          Linear-124            [-1, 197, 3072]       2,362,368\n",
      "            GELU-125            [-1, 197, 3072]               0\n",
      "         Dropout-126            [-1, 197, 3072]               0\n",
      "          Linear-127             [-1, 197, 768]       2,360,064\n",
      "         Dropout-128             [-1, 197, 768]               0\n",
      "     ResidualAdd-129             [-1, 197, 768]               0\n",
      "       LayerNorm-130             [-1, 197, 768]           1,536\n",
      "          Linear-131            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-132          [-1, 8, 197, 197]               0\n",
      "          Linear-133             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-134             [-1, 197, 768]               0\n",
      "         Dropout-135             [-1, 197, 768]               0\n",
      "     ResidualAdd-136             [-1, 197, 768]               0\n",
      "       LayerNorm-137             [-1, 197, 768]           1,536\n",
      "          Linear-138            [-1, 197, 3072]       2,362,368\n",
      "            GELU-139            [-1, 197, 3072]               0\n",
      "         Dropout-140            [-1, 197, 3072]               0\n",
      "          Linear-141             [-1, 197, 768]       2,360,064\n",
      "         Dropout-142             [-1, 197, 768]               0\n",
      "     ResidualAdd-143             [-1, 197, 768]               0\n",
      "       LayerNorm-144             [-1, 197, 768]           1,536\n",
      "          Linear-145            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-146          [-1, 8, 197, 197]               0\n",
      "          Linear-147             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-148             [-1, 197, 768]               0\n",
      "         Dropout-149             [-1, 197, 768]               0\n",
      "     ResidualAdd-150             [-1, 197, 768]               0\n",
      "       LayerNorm-151             [-1, 197, 768]           1,536\n",
      "          Linear-152            [-1, 197, 3072]       2,362,368\n",
      "            GELU-153            [-1, 197, 3072]               0\n",
      "         Dropout-154            [-1, 197, 3072]               0\n",
      "          Linear-155             [-1, 197, 768]       2,360,064\n",
      "         Dropout-156             [-1, 197, 768]               0\n",
      "     ResidualAdd-157             [-1, 197, 768]               0\n",
      "       LayerNorm-158             [-1, 197, 768]           1,536\n",
      "          Linear-159            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-160          [-1, 8, 197, 197]               0\n",
      "          Linear-161             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-162             [-1, 197, 768]               0\n",
      "         Dropout-163             [-1, 197, 768]               0\n",
      "     ResidualAdd-164             [-1, 197, 768]               0\n",
      "       LayerNorm-165             [-1, 197, 768]           1,536\n",
      "          Linear-166            [-1, 197, 3072]       2,362,368\n",
      "            GELU-167            [-1, 197, 3072]               0\n",
      "         Dropout-168            [-1, 197, 3072]               0\n",
      "          Linear-169             [-1, 197, 768]       2,360,064\n",
      "         Dropout-170             [-1, 197, 768]               0\n",
      "     ResidualAdd-171             [-1, 197, 768]               0\n",
      "          Reduce-172                  [-1, 768]               0\n",
      "       LayerNorm-173                  [-1, 768]           1,536\n",
      "          Linear-174                 [-1, 1000]         769,000\n",
      "================================================================\n",
      "Total params: 86,415,592\n",
      "Trainable params: 86,415,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 364.33\n",
      "Params size (MB): 329.65\n",
      "Estimated Total Size (MB): 694.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "        \n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, embed_dim: int = 768, n_classes: int = 1000):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction=\"mean\"),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, n_classes)\n",
    "        )\n",
    "        \n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(self, \n",
    "                 in_channels: int = 3,\n",
    "                 patch_size: int = 16,\n",
    "                 embed_dim: int = 768,\n",
    "                 img_size: int = 224,\n",
    "                 depth: int = 12,\n",
    "                 n_classes: int = 1000,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, embed_dim, img_size),\n",
    "            TransformerEncoder(depth, embed_dim=embed_dim, **kwargs),\n",
    "            ClassificationHead(embed_dim, n_classes)\n",
    "        )\n",
    "        \n",
    "summary(ViT(), (3, 224, 224), device='cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
